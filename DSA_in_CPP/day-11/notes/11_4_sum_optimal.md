# üöÄ Optimal Approach for 4-Sum

## ‚≠ê First ‚Äî what is ‚Äúoptimal‚Äù here?

Better approach was:

‚úî sort
‚úî fix 2 numbers
‚úî 2 pointers to find remaining 2

Time = **O(n¬≥)**

This is already optimal in terms of asymptotic complexity.

So what more can we improve?

‚Üí **Pruning**
‚Üí **Early termination**
‚Üí **Skipping unnecessary branches**

This makes the algorithm drastically faster in real life (though still O(n¬≥) worst-case).

---

---

# üß† Optimal Approach ‚Äî Core Thought Process

Same structure as better approach:

### Step-1

Sort the array

### Step-2

Pick `i` (first index)

### Step-3

Pick `j` (second index)

### Step-4

Use **two pointers** (`l` and `r`)
to find remaining 2 numbers.

---

Up to this point ‚Üí *same as better.*

Now what makes it optimal?

---

---

# üî• OPTIMAL THINKING (Very Important)

We use mathematical reasoning to **stop useless exploration early**, using bounding.

Because the array is sorted!

So we can PREDICT the minimum possible sum and maximum possible sum.

---

## üü• Optimization-1

### Early break when minimum possible sum is already > target

If:

\[
nums[i] + nums[j] + nums[j+1] + nums[j+2] > target
\]

then,
remaining sums will only increase

so we break early.

---

## üü¶ Optimization-2

### Early continue when maximum possible sum is still < target

If

\[
nums[i] + nums[j] + nums[n-1] + nums[n-2] < target
\]

even the 4 biggest values can't reach target

so we continue to next j.

---

## üü© Optimization-3

### Duplicate skipping on all 4 layers

* skip duplicate `i`
* skip duplicate `j`
* skip duplicate left-pointer
* skip duplicate right-pointer

---

## üü® Optimization-4

### Tight pointer movement rules

When:

sum < target ‚Üí move `l`
sum > target ‚Üí move `r`

No random scanning.

---

---

# üìå Why this is optimal?

Because:

* we explore ONLY meaningful combinations
* we avoid checking useless values
* we stop early when bound proves impossibility
* sorting allows mathematical guarantees

With large arrays,
this becomes MUCH faster in practice.

---

---

# üßÆ Time Complexity

### Worst Case:

\[
O(n^3)
\]

same as better approach

BUT with early breaking in real world cases.

So effective is often:

\[
O(n^2 \log n)
\quad\text{to}\quad
O(n^2)
\]

in practice.

But **theoretical** worst-case = (O(n^3))

---

---

# üì¶ Space Complexity

We use:

* constant variables
* pointers only

So:

\[
O(1)
\]

(excluding output)

---

---

# üüß How to THINK like this generally (important lesson)

Whenever problem asks:

> find all combinations of **k numbers** forming a target sum

you should think:

* reduce k-sum to (k-1)-sum
* sort & use pointers
* prune impossible branches

General pattern:

| k-Sum | Strategy           |
| ----- | ------------------ |
| 2     | 2 pointers         |
| 3     | fix 1 ‚Üí 2 pointers |
| 4     | fix 2 ‚Üí 2 pointers |
| 5     | fix 3 ‚Üí 2 pointers |

and so on‚Ä¶

This is a universal template.

---
---
---
---


**‚Äúhow does a human mind even THINK toward such a solution?‚Äù**

Let me explain how to *develop the thinking*, not just memorize solutions.

---

---

# üß† First: Observations & Intuition

When a problem asks:

> find combinations of k numbers whose sum equals target

Your brain must IMMEDIATELY notice:

‚úî Combination
‚úî Sum
‚úî Target
‚úî Multiple elements
‚úî Unique results

This ALWAYS implies:

### ‚Üí Deduplication

### ‚Üí Sorting

### ‚Üí Structured scanning

### ‚Üí Controlled searching

That‚Äôs the mindset foundation.

---

---

# üß≠ Step 1 ‚Äî Recognize PAST patterns

You KNOW 2-sum
You KNOW 3-sum

So your brain asks:

> How did I solve 3-sum?

3-sum thinking:

**Sort + fix 1 + 2 pointers**

Then your brain asks:

> Can this scale geometrically?

Yes:
For 4-sum ‚Üí fix 2 + 2 pointers
For 5-sum ‚Üí fix 3 + 2 pointers
For 6-sum ‚Üí fix 4 + 2 pointers
(and so on)

So the approach is a **generalization**, not invention.

This is key.

---

---

# üß≠ Step 2 ‚Äî understand problem structure

4-sum means:

\[
a + b + c + d = target
\]

Your mind rewrites it as:

\[
a + b + (c + d) = target
\]

Now:

* a and b can be chosen freely
* c + d must be found efficiently

EFFICIENT way to find 2 numbers?

‚Üí two pointers.

This logic is not random.
It is **derived**.

---

---

# üß≠ Step 3 ‚Äî Identify wasteful brute force patterns

Brute does:

```
i, j, k, l loops
```

Meaning:

‚Üí scanning even when it‚Äôs impossible to reach target

Brain asks:

> how can I avoid useless heavy scanning?

Answer:

Use **sorted information**.

---

---

# üß≠ Step 4 ‚Äî THINK about bounds

Since sorted:

Minimum future sum is known

Maximum future sum is known

This allows elimination.

This is not magic,
this is logic.

---

---

# üß≠ Step 5 ‚Äî Notice duplicates will explode answers

So brain asks:

> How can I avoid repeat work?

Solution:

* skip duplicate i
* skip duplicate j
* skip duplicate left
* skip duplicate right

Again,
not magic.

This happens in ALL k-sum problems.

---

---

# üß≠ Step 6 ‚Äî Create a **search window**

Think like this:

Instead of searching the ENTIRE remaining array,
shrink it every iteration.

This is two-pointer scanning.

This principle drives:

* binary search
* windowing
* DP pruning
* greedy
* etc.

---

---

# üí° So the REAL mental path is:

1. sorted array = structured space
2. reduce k-sum to (k-1)-sum
3. reduce to 2-sum eventually
4. use 2 pointers at the final stage
5. shrink search window
6. detect impossible branches early
7. skip duplicates

No magic.
No guessing.
No luck.

100% reasoning.

---

---

# üå± Final lesson

You **grow cognitive tools**, not just collect answers.

Eventually,
when you SEE a problem,
your BRAIN will AUTOMATICALLY say:

* ‚ÄúI can sort‚Äù
* ‚ÄúI can prune these branches‚Äù
* ‚ÄúI can 2-pointer the back half‚Äù
* ‚ÄúI can skip duplicates‚Äù
* ‚ÄúI can eliminate impossible ranges early‚Äù

Because you‚Äôve built a framework.

Not because you memorized.

---

---








---
---

# üß† What is Early Breaking / Pruning?

It means:

**Before even entering inner loops**
we check if continuing is mathematically USELESS.

If we detect:
‚ÄúNO POSSIBLE WAY to reach target from here‚Äù

We QUIT EARLY.

That is pruning.

We cut unnecessary loops.

We stop computation early.

---

---

# üß† Why can we do this in 4-Sum?

**ONLY because array is sorted.**

Because if sorted:
we can mathematically know:

* the *minimum possible future sum*
* the *maximum possible future sum*

before checking all combinations.

---

---

# üß† The Core Idea

Let array be sorted.

Suppose we fixed indices:

`j` and `i`

Now before running two pointer,
we compare:

---

## ‚úî MINIMUM POSSIBLE sum

Compute:

\[
\text{minSum} = nums[j] + nums[i] + nums[i+1] + nums[i+2]
\]

(This is the smallest 4 we can make starting here.)

If:

\[
\text{minSum} > target
\]

Then:

**even smallest possible is greater than target
‚Üí no point continuing
‚Üí break inner loop entirely**

---

## ‚úî MAXIMUM POSSIBLE sum

Compute:

\[
\text{maxSum} = nums[j] + nums[i] + nums[n-1] + nums[n-2]
\]

(This is the largest 4 we can form)

If:

\[
\text{maxSum} < target
\]

Then:

**even largest possible is less than target
‚Üí no point checking this combination
‚Üí skip this i and move forward**

---

---

# üß† WHY this helps?

Because instead of scanning every l,r for impossible cases‚Ä¶

You **SKIP** hundreds or thousands of cycles
in one decision.

Real inputs:
(multiset arrays, large ranges, useless prefixes)

this optimization is HUGE.

---

---

# üß† How do we THINK of this?

This is the important part:

## ‚ö† KEY MENTAL SHIFT:

**STOP thinking in code**

START thinking in bounds.

What are bounds?

‚Üí lowest possible sum
‚Üí highest possible sum

This is mathematical optimization mindset.

Not coding mindset.

---

Here is EXACTLY how someone discovers it:

### Step-1

Ask yourself:

> ‚ÄúIf I fix j and i,
> what is the smallest sum I could possibly make after that?‚Äù

Answer:

use next 2 smallest numbers.

---

### Step-2

Ask:

> ‚ÄúIf even smallest exceeds target,
> why waste time scanning?‚Äù

‚Üí break.

---

### Step-3

Ask:

> ‚ÄúWhat‚Äôs the largest sum I can POSSIBLY reach?‚Äù

use last 2 elements.

---

### Step-4

If even largest sum < target

‚Üí skip this iteration.

---

This is the entire reasoning.

---

---

# üß† When does this general idea apply?

Whenever:

1. array is sorted
2. you need sum
3. you fix some indices
4. you can predict remaining lower/upper bounds

This applies to:

* 2 sum already
* 3 sum optimal versions
* 4 sum
* K sum
* subset search
* backtracking DP
* branch+bound
* knapsack pruning
* DFS optimization
* binary search pruning
* tree DP pruning

This is a HIGH LEVEL SKILL.

Over time,
your brain starts **seeing bounds automatically**.

---

---

# ‚≠ê Key Takeaway

Early Pruning is born from 2 ideas:

### üí° Idea 1:

Sorted array means predictable ordering.

### üí° Idea 2:

Always check whether work is USELESS before doing it.



---
---
---
---

I'll use one sorted example array, walk through a few `a,b` (two fixed indices) cases, compute the exact `minSum` and `maxSum` checks, show whether we **skip** or **break**, and explain **why** that is logically valid.

---

## Example (sorted)

```
nums = [-10, -4, -1, 0, 1, 2, 5, 9]
n = 8
target = 3
```

We use indices (a<b<l<r). After fixing (a) and (b), normally we would run the two-pointer loop on ([l=b+1 \dots r=n-1]). Pruning lets us **skip** that two-pointer work when it's impossible.

### Definitions (for fixed (a,b))

\[
\text{minSum} = nums[a] + nums[b] + nums[b+1] + nums[b+2]
\]
(the smallest 4-sum you can make with these (a,b))

\[
\text{maxSum} = nums[a] + nums[b] + nums[n-1] + nums[n-2]
\]
(the largest 4-sum you can make with these (a,b))

Pruning rules:

* If $(\text{minSum} > \text{target})$ ‚Üí **break** the inner loop over (b) (for this (a) larger (b) only increases minSum).
* If (\text{maxSum} < \text{target}) ‚Üí **continue** to next (b) (even the largest possible is too small).

---

## Walkthrough: compute values and decide

### 1) (a=0) ‚Üí (nums[a]=-10)

* (b=1) ‚Üí (nums[b]=-4)

  \[
  \begin{aligned}
  \text{minSum} &= -10 + (-4) + (-1) + 0 = -15 \quad(<3)\
  \text{maxSum} &= -10 + (-4) + 9 + 5 = 0 \quad(<3)
  \end{aligned}
  \]

  Here (\text{maxSum}<\text{target}). That means **even using the two largest remaining numbers we can't reach 3**. So we **skip** the costly two-pointer entirely for this (b) and do `continue` (move to next (b)).
  *Saved work*: we did not run any two-pointer iterations for this (b).

* (b=2) ‚Üí (nums[b]=-1)

  \[
  \begin{aligned}
  \text{minSum} &= -10 + (-1) + 0 + 1 = -10 \quad(<3)\
  \text{maxSum} &= -10 + (-1) + 9 + 5 = 3 \quad(=;3)
  \end{aligned}
  \]

  Now (\text{maxSum} \ge \text{target}) and (\text{minSum} \le \text{target}) ‚Üí **possible**, so we run the two-pointer and search. (We might find a quadruplet or not.)

* (b=3) ‚Üí (nums[b]=0)

  \[
  \begin{aligned}
  \text{minSum} &= -10 + 0 + 1 + 2 = -7 \quad(<3)\
  \text{maxSum} &= -10 + 0 + 9 + 5 = 4 \quad(>3)
  \end{aligned}
  \]

  Again possible ‚Üí run two-pointer.

(So for (a=0), we **skipped** the heavy inner work at (b=1) only.)

---

### 2) (a=2) ‚Üí (nums[a]=-1)

* (b=3) ‚Üí (nums[b]=0)

  \[
  \begin{aligned}
  \text{minSum} &= -1 + 0 + 1 + 2 = 2 \quad(\le 3)\
  \text{maxSum} &= -1 + 0 + 9 + 5 = 13 \quad(>3)
  \end{aligned}
  \]

  Possible ‚Üí run two-pointer.

* (b=4) ‚Üí (nums[b]=1)

  \[
  \begin{aligned}
  \text{minSum} &= -1 + 1 + 2 + 5 = 7 \quad(>3)\
  \text{maxSum} &= -1 + 1 + 9 + 5 = 14 \quad(>3)
  \end{aligned}
  \]

  Here $(\text{minSum} > \text{target})$. Since the array is sorted, increasing (b) will only increase (nums[b]), hence $(\text{minSum})$ for any larger (b) will be even larger. So we can **break** the loop over (b) entirely for this (a). No point checking (b=5,6,\dots).
  *Saved work*: we do not examine any of the remaining (b) values for (a=2).

This is the classic **break** case.

---

## Why these inequalities are **mathematically valid**

* For $(\text{maxSum} < \text{target})$:

  $(\text{maxSum})$ uses the two largest possible remaining elements ((nums[n-1],nums[n-2])). If their sum with the fixed (a,b) is already < target, **no** pair (l,r) can raise the sum to target because every other choice gives an equal or smaller sum. So entering the two-pointer loop is pointless.

* For $(\text{minSum} > \text{target})$:

  $(\text{minSum})$ uses the two smallest remaining elements ((nums[b+1],nums[b+2])). If even this smallest-case sum exceeds target, then **every** pair (l,r) (which will use elements ‚â• those) gives a larger sum, so nothing will match target. Moreover, because the array is sorted, increasing (b) increases (nums[b]) (so minSum for larger (b) is only larger). Thus you can **break** the (b) loop.

Both arguments depend only on monotonic ordering ‚Äî they are airtight.

---

## Show how much work we saved (rough estimate)

Assume naive scanning for a given (a,b) would run two-pointer with about (O(m)) pointer moves where (m) is size of remaining slice. If we skip one (b) by `maxSum < target`, we save ~(m) pointer steps. If we break at some (b) due to `minSum > target`, we save scanning for **all larger (b)**, potentially saving dozens or hundreds of two-pointer runs depending on array size.

In our small example, for (a=0, b=1) we skip one full two-pointer run (which may be ~5‚Äì6 pointer steps). For (a=2), when we break at (b=4) we avoid two-pointer runs for (b=4,5,6) ‚Äî several whole inner searches avoided.

On big arrays (n in thousands) these savings multiply: a single break can skip (O(n)) inner-`b` iterations, each of which would have cost (O(n)) ‚Äî that‚Äôs potentially (O(n^2)) saved in practice (though worst-case complexity remains (O(n^3))).

---

## Where to place these checks in code (pseudo)

Place them **immediately after** you pick (a) and (b), before starting the two-pointer.

```text
for a in 0..n-4:
  if a>0 and nums[a]==nums[a-1]: continue    // duplicate skip for a
  for b in a+1..n-3:
    if b>a+1 and nums[b]==nums[b-1]: continue  // duplicate skip for b

    minSum = nums[a] + nums[b] + nums[b+1] + nums[b+2]
    if minSum > target: 
       break        // for this 'a', further b will only increase minSum

    maxSum = nums[a] + nums[b] + nums[n-1] + nums[n-2]
    if maxSum < target:
       continue     // this 'b' can't reach target even with largest remaining pairs

    // else: run two-pointer on l=b+1..r=n-1
```

---

## Final intuitive statement

Pruning is simply **using sorted order to compute tight upper/lower bounds** for what *can* happen next. If the bounds show impossibility, we cut work immediately. You can always *prove* correctness because the bounds are extremes (smallest or largest possible sums) and ordering guarantees monotonic behavior when indices increase.

Once you practice this thinking ‚Äî ask ‚Äúwhat‚Äôs the minimum possible sum from here?‚Äù and ‚Äúwhat‚Äôs the maximum possible sum from here?‚Äù ‚Äî pruning becomes natural. It‚Äôs just short-circuiting impossible branches by math.
